---
title: "Tarea 8"
author: "Francisco Paz"

output: html_document
---


Considera la siguiente red para un problema de clasificación binaria:

```{r}
library(igraph)
 gr <- graph(c(1,2,1,3,2,4,3,4))
 plot(gr, layout = matrix(c(-2,0,0,1,0,-1,2,0), byrow=T, ncol=2),
      vertex.label=c('X1','a_1','a_2','p1'), 
      vertex.size=50, vertex.color='salmon', vertex.label.cex=1.5,
   vertex.label.color='white',vertex.frame.color=NA,
    edge.label=c(0.2,-0.3,-1,2)
   )
```

 Supón que los sesgos son 0 para la unidad $a_1$, 0 para la unidad $a_2$ y -0.5 para la unidad $p$.

1. Escribe cada $\theta_{i,j}^{(l)}$ según la notación de clase e identifica su valor. Por ejemplo, tenemos que $\theta^{(1)}_{1,0} = 0$ (tienes que escribir 7 valores como este). Escribe las matrices $\Theta^{(1)}$ y $\Theta^{(2)}$

$$\theta^{(1)}_{1,0} = 0  \ , \ \  \theta^{(1)}_{1,1} = 0.2  \ , \ \ \theta^{(1)}_{2,0} = 0  \ \ \  \theta^{(1)}_{2,1} = -0.3$$

$$a^{(1)}_1 = h(0 + 0.2 \ x_1 )$$
$$a^{(1)}_2 = h(0 - 0.3 \ x_1 )$$

$$\theta^{(2)}_{1,0} = -0.5  \ , \ \  \theta^{(2)}_{1,1} = -1  \ , \ \  \theta^{(2)}_{1,2} = 2$$
$$p_1 = h(-0.5 -a_1 + 2 a_2) $$

La matriz $\Theta_1$ es:

$$\begin{bmatrix}
    0 & 0.2 \\
    0 & -0.3
\end{bmatrix}$$


La matriz $\Theta_2$ es:
$$\begin{bmatrix}
    -0.5 & -1 &2
\end{bmatrix}$$

2. Supón que tenemos un caso (observación) con $(x, y)=(1, 0)$. ¿Qué es $a_1^{(1)}$? Haz forward feed para calcular los valores de $a_1^{(2)}$, $a_2^{(2)}$ y $p=a_1^{(3)}$.

$a_1 = 1$

```{r}
h <- function(z) exp(z)/(1+exp(z))

a1 <- h(0 + 0.2*1)
a2 <- h(0 - 0.3*1)
a_1_3 <- h(-0.5 -a1 + 2*a2)

a1
a2
a_1_3
```


3. Calcula la devianza para el caso $(x, y)=(1, 0)$

```{r}
p <- a_1_3
y <- 0

-2*(y * log(p) + (1 - y) * log(1 - p))

```

4. Según el cálculo que hiciste en 2 y 3, intuitivamente, ¿qué conviene hacer con los dos últimos pesos de la última capa para reducir la devianza? ¿Incrementarlos o disminuirlos? 

Dismunuir.


5. Según tu cálculo de 2, y 3, ¿qué conviene hacer con las unidades $a_1^{(2)}$, $a_2^{(2)}$  para reducir la devianza? 

Incrementar a1 y reducir a2

6. Usando el inciso anterior, contesta qué conviene hacer con los pesos $\Theta^{(1)}$ para reducir la devianza.

Hcaer pequeño el renglon 1  e incrementar el renglon 2-

7. Error en la última capa (3): Calcula $\delta^{(3)}_1$

```{r}
delta_3_1 <- p-y
delta_3_1
``` 

8. Ahora calcula (ver algoritmo de backpropagation) la derivada $\frac{\partial D}{\partial \theta^{(2)}_{1,1}}$. El resultado coincide con tu intuición del inciso 4? Puedes intentar calcular directamente esta derivada también, con el método que quieras.

```{r}
delta_3_1*a1
```

9. ¿Cómo se calculan las derivadas de los sesgos? Calcula $\frac{\partial D}{\partial \theta^{(2)}_{1,0}}$ usando  el inciso anterior.


```{r}
delta_3_1*1
```

10. Paso de backpropagation (error en capa 2): Calcula $\delta^{(2)}_1$ y $\delta^{(2)}_2$, según la fórmula que vimos en las notas  (puedes usar la matricial o la coordenada a coordenada).

Notar que para theta, quitamos el sesgo de los valores y por eso solo queda "-1   +2" y no "-0.5  -1  +2"
```{r}
da21 <- h(0 + .2*1)*(1-h(0  + .2*1))
da22 <- h(0 - 0.3*1)*(1-h(0 - 0.3*1))
hs<-c(da21,da22)
theta<- c(-1,2)
delta_2 <- t(theta)*delta_3_1%*%hs
delta_2
```


$$\begin{bmatrix}
    0 & 0.2 \\
    0 & -0.3
\end{bmatrix}$$


```{r}
prod<-c(0, 0.2*delta_2[1] - 0.3*delta_2[2])
delta_1<-(prod[1] + prod[2] )* h(1)
delta_1
```


11. ¿Qué indican los valores de $\delta^{(2)}_1, \delta^{(2)}_2$? Compara con tu respuesta del inciso 5.

También se conocen como errores de las unidades y nos indican hacia donde movernos.

12. Ahora utiliza $\delta^{(2)}$ para calcular  $\frac{\partial D}{\partial \theta_{1,0}^{(1)}}$, $\frac{\partial D}{\partial \theta_{1,1}^{(1)}}$, $\frac{\partial D}{\partial \theta_{2,0}^{(1)}}$ y $\frac{\partial D}{\partial \theta_{2,1}^{(1)}}$ . 

```{r}
delta_2[1]*1
delta_2[1]*1

delta_2[2]*1
delta_2[2]*1
```

13. ¿Puedes explicar intuitivamente los signos que obtuviste para las derivadas del inciso anterior (tip: tienes qué ver también que sucede en la siguiente capa)?


    